# ScrapeScout

ScrapeScout is a **Flaskâ€‘based job aggregation platform** that collects vacancies from multiple Georgian job boards (starting with *jobs.ge*) and presents them through a single, modern interface. It scrapes data, stores it in a database, and lets users search, preview and bookmark listings â€“ all inside a Dockerised stack that runs the same locally and in the cloud.

---

## Key Features

* **Multiâ€‘site scraping** with Selenium + BeautifulSoup (dynamic pages fully rendered).
* **Scheduled indexing** every six hours via APScheduler.
* **PostgreSQL / SQLite storage** accessed through SQLAlchemy models.
* **Fullâ€‘text search & rich filters** (region, city, category, keyword, sort).
* **User accounts** powered by Flaskâ€‘Login and bcrypt password hashing.
* **Bookmark system** â€“ save jobs to a favourites list.
* **Responsive UI** with darkâ€‘mode toggle, realâ€‘time preview panel and email copy helper.
* **Containerised deployment** â€“ one `docker compose up` launches web + db.
* **Render.com ready** â€“ configuration file provided for oneâ€‘click cloud deploy.

---

## Technology Stack

* **Backend:** PythonÂ 3.11, Flask, SQLAlchemy, APScheduler
* **Scraping:** Selenium (headless Chrome), BeautifulSoupÂ 4
* **Frontend:** HTMLÂ 5, Jinja2 templates, vanilla JavaScript, FontÂ Awesome, CSSâ€¯variables
* **Database:** PostgreSQL in production, SQLite for local quickâ€‘start
* **Auth & Security:** Flaskâ€‘Login, bcrypt, CSRF protection via Flaskâ€‘WTF
* **Containerisation:** Docker, DockerÂ Compose, Gunicorn

---

## Quick Start (Docker)

```bash
git clone https://github.com/GiorgiBokuchava/ScrapeScout.git
cd ScrapeScout
cp .env.example .env            # edit values if desired
docker compose up --build       # launches web (port 5000) and db (port 5432)
```

Open [http://localhost:5000](http://localhost:5000) in your browser.

### Required Environment Variables

* `SECRET_KEY`Â â€“ any random string for Flask sessions.
* `DATABASE_URL`Â â€“ connection string for Postgres (`postgresql://user:pass@db:5432/scrapescout`).
* `SELENIUM_DRIVER`Â â€“ path or command for headless Chrome (defaults work in the container).

The provided **`.env.example`** lists sensible defaults for local development.

---

## Running Without Docker

```bash
python -m venv env && source env/bin/activate
pip install -r requirements.txt
export SECRET_KEY="dev-secret"
export DATABASE_URL="sqlite:///instance/scrape.db"
flask db upgrade
python run.py
```

The app starts on [http://localhost:5000](http://localhost:5000). Selenium requires a local Chrome/Chromium + chromedriver in `$PATH`.

---

## Project Layout (abridged)

```
ScrapeScout/
â”‚  run.py                    # entrypoint
â”‚  docker-compose.yaml       # dev stack (web + db)
â”‚  Dockerfile                # builds the web image
â”‚  render.yaml               # Render.com service definition
â”œâ”€ application/
â”‚   â”œâ”€ models.py             # User, Job ORM models
â”‚   â”œâ”€ jobs_ge.py            # scraper for jobs.ge
â”‚   â”œâ”€ scrape.py             # indexing & query helpers
â”‚   â”œâ”€ routes.py             # Flask controllers
â”‚   â”œâ”€ forms.py              # WTForms classes
â”‚   â””â”€ scheduler.py          # APScheduler setup
â”œâ”€ templates/                # Jinja2 HTML files
â”œâ”€ static/                   # JS, CSS, SVG assets
â”œâ”€ migrations/               # Alembic versions
â””â”€ README.md
```

---

## Contributing

1. Fork the repo and create a new branch (e.g. `feature/new-board`).
2. Commit your changes with clear messages.
3. Push and open a Pull Request describing the enhancement.

Please run `black` for formatting and ensure `pytest` passes before submitting.

---

## License

This project is released under the **MIT License**. See `LICENSE` for full text.

---

## Contact

* GitHub: [https://github.com/GiorgiBokuchava](https://github.com/GiorgiBokuchava)
* Email: listed in GitHub profile

Happy scraping! ðŸš€
